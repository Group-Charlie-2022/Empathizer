{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Seq2Seq Formality Transfer Model\n",
    "\n",
    "References:\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "- https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Experiment with using GloVe word embedding rather than one-hot character encoding\n",
    "- Experiment with one-hot n-gram embeddings as well\n",
    "- Experiment with LSTM/GRU effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, TimeDistributed, RepeatVector, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1024\n",
    "train_prop = 0.9\n",
    "checkpoints_dir = os.path.abspath(\"keras-checkpoints\")\n",
    "\n",
    "with open(\"labelled.txt\", \"r\") as l:\n",
    "    raw_data = l.read()\n",
    "lines = [x.strip() for x in raw_data.split(\"\\n\")]\n",
    "max_length = max(len(i) for i in lines)\n",
    "\n",
    "pairs = [(lines[i], lines[i+1]) for i in range(0, min(len(lines),num_samples*2), 2)]\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x).strip()\n",
    "\n",
    "ctable = CharacterTable(raw_data.replace(\"\\n\", \"\")+\"\\x00\")\n",
    "\n",
    "X = np.zeros((len(pairs), max_length, len(ctable.chars)))\n",
    "Y = np.zeros((len(pairs), max_length, len(ctable.chars)))\n",
    "\n",
    "for i, (xi, yi) in enumerate(pairs):\n",
    "    X[i] = ctable.encode(xi, max_length)\n",
    "    Y[i] = ctable.encode(yi, max_length)\n",
    "\n",
    "train_num = int(train_prop * len(pairs))\n",
    "\n",
    "X_train, X_test = X[:train_num], X[train_num:]\n",
    "Y_train, Y_test = Y[:train_num], Y[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "\n",
    " # Maybe replace GRUs with LSTMs (better performance but slower)\n",
    "model = Sequential()\n",
    "model.add(GRU(hidden_size, input_shape=(max_length, len(ctable.chars))))\n",
    "model.add(RepeatVector(max_length))\n",
    "model.add(GRU(hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(len(ctable.chars), activation=\"softmax\")))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter(Callback):\n",
    "    id_counter = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.id = f\"plot-{Plotter.id_counter}\"\n",
    "        Plotter.id_counter += 1\n",
    "        self.fig, self.ax = plt.subplots(figsize=(24, 6), dpi=80)\n",
    "        self.ax.set_xlabel(\"Epoch\")\n",
    "        self.ax.set_ylabel(\"Loss\")\n",
    "        self.ax.xaxis.set_major_locator(ticker.MultipleLocator(base=10.0))\n",
    "        self.ax.xaxis.set_minor_locator(ticker.MultipleLocator(base=1.0))\n",
    "        self.ax.yaxis.set_major_locator(ticker.MultipleLocator(base=0.1))\n",
    "        self.ax.yaxis.set_minor_locator(ticker.MultipleLocator(base=0.02))\n",
    "        self.ax.grid(which=\"major\", color=\"#888888\")\n",
    "        self.ax.grid(which=\"minor\", color=\"#bbbbbb\")\n",
    "\n",
    "        self.fig.patch.set_facecolor(\"white\")\n",
    "        box = self.ax.get_position()\n",
    "        self.ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "        display.display(self.fig, display_id=self.id)\n",
    "\n",
    "        self.epochs = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for l in self.ax.lines:\n",
    "            l.remove()\n",
    "        \n",
    "        self.epochs.append(epoch)\n",
    "        self.train_losses.append(logs[\"loss\"])\n",
    "        self.val_losses.append(logs[\"val_loss\"])\n",
    "        \n",
    "        t_line, = self.ax.plot(self.epochs, self.train_losses, c=\"#55CDFC\")\n",
    "        v_line, = self.ax.plot(self.epochs, self.val_losses, c=\"#F7A8B8\")\n",
    "        self.ax.autoscale()\n",
    "        self.ax.set_ylim(0, None)\n",
    "        self.ax.legend([t_line, v_line], [\"Training\", \"Validation\"], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        display.update_display(self.fig, display_id=self.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Existing Checkpoints &mdash; WARNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(checkpoints_dir):\n",
    "    os.makedirs(checkpoints_dir)\n",
    "\n",
    "for f in os.listdir(checkpoints_dir):\n",
    "    os.unlink(os.path.join(checkpoints_dir, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 100\n",
    "\n",
    "demo_str = \"You should seek advice from a medical professional.\"\n",
    "demo_vec = np.array([ctable.encode(demo_str, max_length)])\n",
    "\n",
    "filepath = os.path.join(checkpoints_dir, \"formal2casual-{epoch:02d}-{loss:.4f}.ckpt\")\n",
    "checkpoint = ModelCheckpoint(filepath, save_weights_only=True, monitor=\"loss\", verbose=0, save_best_only=True, mode=\"min\")\n",
    "\n",
    "class PrintDemo(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:\n",
    "            return\n",
    "        pred_vec = self.model.predict(demo_vec, verbose=0)\n",
    "        pred_str = ctable.decode(pred_vec[0])\n",
    "\n",
    "        print(f\"\\\"{demo_str}\\\" -> \\\"{pred_str}\\\"\\n\")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=[checkpoint, PrintDemo(), Plotter()]\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoints_dir)\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"You should seek advice from a medical professional.\"\n",
    "start = time.time()\n",
    "test_vec = np.array([ctable.encode(demo_str, max_length)])\n",
    "\n",
    "pred_vec = model.predict(test_vec, verbose=0)\n",
    "pred_str = ctable.decode(pred_vec[0])\n",
    "elapsed = time.time() - start\n",
    "print(test_str)\n",
    "print(\"->\")\n",
    "print(pred_str)\n",
    "print(f\"Took {elapsed:5f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe9152bf2f574327d6e225fe0caf6c004889fb08c66ba1e27720217847b14197"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
