{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Seq2Seq Formality Transfer Model\n",
    "\n",
    "References:\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "- https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py\n",
    "- https://github.com/lukas/ml-class/blob/master/videos/cnn-text/imdb-embedding.py\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Experiment with LSTM/GRU effectiveness\n",
    "- Experiment with attention\n",
    "- Probabilistic sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, TimeDistributed, RepeatVector, Dense, Lambda, Softmax\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import zipfile\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = None\n",
    "\n",
    "with open(\"labelled.txt\", \"r\") as l:\n",
    "    raw_data = l.read()\n",
    "lines = [x.strip() for x in raw_data.split(\"\\n\")]\n",
    "max_length = max(len(i) for i in lines)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, len(lines)-1 if num_samples is None else min(len(lines)-1,num_samples*2), 2):\n",
    "    X.append(lines[i])\n",
    "    Y.append(lines[i+1])\n",
    "\n",
    "\n",
    "train_prop = 0.9\n",
    "train_num = int(train_prop * len(X))\n",
    "\n",
    "X_train, X_test = X[:train_num], X[train_num:]\n",
    "Y_train, Y_test = Y[:train_num], Y[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/projects/glove/\n",
    "glove_zip = \"glove.6B.zip\"\n",
    "glove_txt = \"glove.6B.50d.txt\"\n",
    "vocab_size = 40000\n",
    "embedding_dim = 50\n",
    "\n",
    "if not os.path.isfile(glove_txt):\n",
    "    pbar = None\n",
    "    def show_progress(block_num, block_size, total_size):\n",
    "        global pbar\n",
    "        if pbar is None:\n",
    "            pbar = progressbar.ProgressBar(maxval=total_size)\n",
    "            pbar.start()\n",
    "\n",
    "        downloaded = block_num * block_size\n",
    "        if downloaded < total_size:\n",
    "            pbar.update(downloaded)\n",
    "        else:\n",
    "            pbar.finish()\n",
    "            pbar = None\n",
    "\n",
    "    url = f\"http://nlp.stanford.edu/data/{glove_zip}\"\n",
    "    urllib.request.urlretrieve(url, glove_zip, show_progress)\n",
    "    with zipfile.ZipFile(glove_zip, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X_train + Y_train)\n",
    "\n",
    "X_train_arr = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_arr = tokenizer.texts_to_sequences(X_test)\n",
    "Y_train_arr = tokenizer.texts_to_sequences(Y_train)\n",
    "Y_test_arr = tokenizer.texts_to_sequences(Y_test)\n",
    "\n",
    "X_train_arr = sequence.pad_sequences(X_train_arr, maxlen=max_length)\n",
    "X_test_arr = sequence.pad_sequences(X_test_arr, maxlen=max_length)\n",
    "Y_train_arr = sequence.pad_sequences(Y_train_arr, maxlen=max_length)\n",
    "Y_test_arr = sequence.pad_sequences(Y_test_arr, maxlen=max_length)\n",
    "\n",
    "embeddings_index = dict()\n",
    "with open(glove_txt, encoding=\"utf8\") as f:\n",
    "    for line in f.read().split(\"\\n\"):\n",
    "        if line == \"\":\n",
    "            break\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=\"float32\")\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocab_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "embedding_matrix_inverse = tf.linalg.pinv(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 600, 50)           2000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               314368    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 600, 256)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 600, 256)          525312    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 600, 50)          12850     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 600, 40000)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,852,530\n",
      "Trainable params: 852,530\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "checkpoints_dir = os.path.abspath(\"keras-checkpoints\")\n",
    "\n",
    "# (None, max_length) ->\n",
    "# (None, max_length, embedding_dim) ->\n",
    "# (None, hidden_dim) ->\n",
    "# (None, max_length, hidden_dim) ->\n",
    "# (None, max_length, hidden_dim) ->\n",
    "# (None, max_length, embedding_dim) ->\n",
    "# (None, max_length, vocab_size) ->\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer:=Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(hidden_size, input_shape=(max_length, embedding_dim)))\n",
    "model.add(RepeatVector(max_length))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(embedding_dim, activation=\"softmax\")))\n",
    "model.add(Lambda(lambda x: Softmax()(tf.matmul(x,embedding_matrix_inverse)), output_shape=(None, max_length, vocab_size)))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter(Callback):\n",
    "    id_counter = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.id = f\"plot-{Plotter.id_counter}\"\n",
    "        Plotter.id_counter += 1\n",
    "        self.fig, self.ax = plt.subplots(figsize=(24, 6), dpi=80)\n",
    "        self.ax.set_xlabel(\"Epoch\")\n",
    "        self.ax.set_ylabel(\"Loss\")\n",
    "        self.ax.xaxis.set_major_locator(ticker.MultipleLocator(base=10.0))\n",
    "        self.ax.xaxis.set_minor_locator(ticker.MultipleLocator(base=1.0))\n",
    "        # self.ax.yaxis.set_major_locator(ticker.MultipleLocator(base=0.1))\n",
    "        # self.ax.yaxis.set_minor_locator(ticker.MultipleLocator(base=0.02))\n",
    "        self.ax.grid(which=\"major\", color=\"#888888\")\n",
    "        self.ax.grid(which=\"minor\", color=\"#bbbbbb\")\n",
    "\n",
    "        self.fig.patch.set_facecolor(\"white\")\n",
    "        box = self.ax.get_position()\n",
    "        self.ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "        display.display(self.fig, display_id=self.id)\n",
    "\n",
    "        self.max_loss = -1\n",
    "\n",
    "        self.epochs = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for l in self.ax.lines:\n",
    "            l.remove()\n",
    "        \n",
    "        self.epochs.append(epoch)\n",
    "        self.train_losses.append(t_loss:=logs[\"loss\"])\n",
    "        self.val_losses.append(v_loss:=logs[\"val_loss\"])\n",
    "\n",
    "        self.max_loss = max(self.max_loss, t_loss, v_loss)\n",
    "        \n",
    "        t_line, = self.ax.plot(self.epochs, self.train_losses, c=\"#55CDFC\")\n",
    "        v_line, = self.ax.plot(self.epochs, self.val_losses, c=\"#F7A8B8\")\n",
    "        self.ax.autoscale()\n",
    "        self.ax.set_ylim(0, self.max_loss * 1.1)\n",
    "        self.ax.legend([t_line, v_line], [\"Training\", \"Validation\"], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        display.update_display(self.fig, display_id=self.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Existing Checkpoints &mdash; WARNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(checkpoints_dir):\n",
    "    os.makedirs(checkpoints_dir)\n",
    "\n",
    "for f in os.listdir(checkpoints_dir):\n",
    "    os.unlink(os.path.join(checkpoints_dir, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 100\n",
    "\n",
    "demo_str = \"You should seek advice from a medical professional.\"\n",
    "demo_vec = tokenizer.texts_to_sequences([demo_str])\n",
    "demo_vec = sequence.pad_sequences(demo_vec, maxlen=max_length)\n",
    "\n",
    "filepath = os.path.join(checkpoints_dir, \"formal2casual-{epoch:02d}-{loss:.4f}.ckpt\")\n",
    "checkpoint = ModelCheckpoint(filepath, save_weights_only=True, monitor=\"loss\", verbose=0, save_best_only=True, mode=\"min\")\n",
    "\n",
    "class PrintDemo(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_time = time.time()\n",
    "        self.last_time = self.start_time\n",
    "    \n",
    "    def format_time(self, seconds):\n",
    "        if seconds < 60:\n",
    "            return f\"{seconds:.2f}s\"\n",
    "        elif seconds < 3600:\n",
    "            return f\"{(seconds/60):.2f}min\"\n",
    "        else:\n",
    "            return f\"{(seconds/3600):.2f}hr\"\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:\n",
    "            return\n",
    "        now = time.time()\n",
    "        elapsed = now-self.start_time\n",
    "        pred_vec = np.argmax(self.model.predict(demo_vec, verbose=0), axis=-1)\n",
    "        pred_str = tokenizer.sequences_to_texts(pred_vec)[0].strip()\n",
    "        max_preview_length = 50\n",
    "        preview = (pred_str[:max_preview_length] + '...') if len(pred_str) > max_preview_length else pred_str\n",
    "\n",
    "        print(f\"Time for Epoch: {self.format_time(now-self.last_time)}, Total Elapsed: {self.format_time(elapsed)}, Total ETA: {self.format_time(elapsed/epoch * epochs)}\")\n",
    "        print(f\"\\\"{demo_str}\\\" -> \\\"{preview}\\\"\\n\")\n",
    "        self.last_time = now\n",
    "\n",
    "model.fit(\n",
    "    X_train_arr,\n",
    "    Y_train_arr,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test_arr, Y_test_arr),\n",
    "    callbacks=[Plotter(), checkpoint, PrintDemo()]\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoints_dir)\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"You should seek advice from a medical professional.\"\n",
    "start = time.time()\n",
    "test_vec = tokenizer.texts_to_sequences([test_str])\n",
    "test_vec = sequence.pad_sequences(test_vec, maxlen=max_length)\n",
    "pred_vec = np.argmax(model.predict(test_vec, verbose=0), axis=-1)\n",
    "\n",
    "pred_str = tokenizer.sequences_to_texts(pred_vec)[0].strip()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(test_str)\n",
    "print(\"->\")\n",
    "print(pred_str)\n",
    "print(f\"Took {elapsed:5f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe9152bf2f574327d6e225fe0caf6c004889fb08c66ba1e27720217847b14197"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
