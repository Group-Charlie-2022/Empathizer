{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently seems as though the models take too much memory to run on the GPU (~8GB)\n",
    "device = \"cpu\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "labelled_data_path = \"labelled.txt\"\n",
    "models_dir = \"models\"\n",
    "\n",
    "# Make sure that these symbols are never in the training data\n",
    "start_symbol = \"<START>\"\n",
    "end_symbol = \"<END>\"\n",
    "start_symbol_index = 0  # These values can't change\n",
    "end_symbol_index = 1   # ^^^\n",
    "\n",
    "max_sentence_length = 10  # This seems to be a limiting factor memory-wise\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_time(seconds):\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        return f\"{(seconds/60):.2f}min\"\n",
    "    else:\n",
    "        return f\"{(seconds/3600):.2f}hr\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6), dpi=80)\n",
    "ax.set_xlabel(\"Iter\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(base=10.0))\n",
    "ax.xaxis.set_minor_locator(ticker.MultipleLocator(base=1.0))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(base=1.0))\n",
    "ax.yaxis.set_minor_locator(ticker.MultipleLocator(base=0.2))\n",
    "ax.grid(which=\"major\", color=\"#888888\")\n",
    "ax.grid(which=\"minor\", color=\"#bbbbbb\")\n",
    "fig.patch.set_facecolor(\"white\")\n",
    "\n",
    "def show_plot(iters, losses):\n",
    "    for l in ax.lines:\n",
    "        l.remove()\n",
    "    ax.plot(iters, losses, c=\"blue\")\n",
    "    ax.autoscale()\n",
    "    ax.set_ylim(0, None)\n",
    "    display.update_display(fig, display_id=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Style:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.words = [start_symbol, end_symbol]\n",
    "        self.word_to_index = {\n",
    "            start_symbol: start_symbol_index, end_symbol: end_symbol_index}\n",
    "        self.word_count = {}\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word_to_index:\n",
    "            self.word_to_index[word] = len(self.words)\n",
    "            self.words.append(word)\n",
    "            self.word_count[word] = 1\n",
    "        else:\n",
    "            self.word_count[word] += 1\n",
    "\n",
    "    def add_sentence(self, sentence, max_length=max_sentence_length):\n",
    "        for word in re.findall(r\"[A-Za-z-']+|[^ ]+\", sentence)[:max_length-1]:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def get_sentence_indices(self, sentence, max_length=max_sentence_length):\n",
    "        return list(self.word_to_index[word] for word in re.findall(r\"[A-Za-z-']+|[^ ]+\", sentence) if word in self.word_to_index)[:max_length-1]\n",
    "\n",
    "\n",
    "def load_styles(style1, style2):\n",
    "    with open(labelled_data_path, \"r\") as l:\n",
    "        lines = [x.strip() for x in l.readlines() if x.strip() != \"\"]\n",
    "\n",
    "    pairs = [(lines[i], lines[i+1]) for i in range(0, len(lines), 2)]\n",
    "\n",
    "    s1 = Style(style1)\n",
    "    s2 = Style(style2)\n",
    "    for x, y in pairs:\n",
    "        s1.add_sentence(x)\n",
    "        s2.add_sentence(y)\n",
    "\n",
    "    return s1, s2, pairs\n",
    "\n",
    "def get_sentence_tensor(style, sentence):\n",
    "    indices = style.get_sentence_indices(sentence)\n",
    "    indices.append(end_symbol_index)\n",
    "    return torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def get_pair_tensors(style1, style2, pair):\n",
    "    x = get_sentence_tensor(style1, pair[0])\n",
    "    desired_y = get_sentence_tensor(style2, pair[1])\n",
    "    return (x, desired_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal, casual, pairs = load_styles(\"formal\", \"casual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        y = embedded\n",
    "        y, hidden = self.gru(y, hidden)\n",
    "        return y, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=max_sentence_length):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)),\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(\n",
    "            0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        y = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        y = self.attn_combine(y).unsqueeze(0)\n",
    "\n",
    "        y = F.relu(y)\n",
    "        y, hidden = self.gru(y, hidden)\n",
    "\n",
    "        y = F.log_softmax(self.out(y[0]), dim=1)\n",
    "        return y, hidden, attn_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, desired_y, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=max_sentence_length):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    x_length = x.size(0)\n",
    "    desired_y_length = desired_y.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(\n",
    "        max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(x_length):\n",
    "        encoder_output, encoder_hidden = encoder(x[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[start_symbol_index]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(desired_y_length):\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input,\n",
    "                decoder_hidden,\n",
    "                encoder_outputs\n",
    "            )\n",
    "            loss += criterion(decoder_output, desired_y[di])\n",
    "            decoder_input = desired_y[di]\n",
    "\n",
    "    else:\n",
    "        for di in range(desired_y_length):\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input,\n",
    "                decoder_hidden,\n",
    "                encoder_outputs\n",
    "            )\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            loss += criterion(decoder_output, desired_y[di])\n",
    "            if decoder_input.item() == end_symbol_index:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / desired_y_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iters(pair_tensors, encoder, decoder, n_iters, print_every=1000, plot_every=100, save_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_iters = []\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    saves = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [random.choice(pair_tensors) for _ in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    print(\"Training...\")\n",
    "    for iter, (x, desired_y) in enumerate(training_pairs):\n",
    "        loss = train(x, desired_y, encoder, decoder,\n",
    "                     encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if (iter + 1) % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"Iter [{iter+1}/{n_iters}] - {(100 * (iter+1)/n_iters):.4f}%, Elapsed: {format_time(elapsed)}, ETA: {format_time(elapsed/(iter+1) * (n_iters - iter - 1))}, Avg loss: {print_loss_avg:.5f}\")\n",
    "\n",
    "        if (iter + 1) % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_loss_total = 0\n",
    "            plot_iters.append(iter+1)\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "\n",
    "            show_plot(plot_iters, plot_losses)\n",
    "        \n",
    "        if (iter + 1) % save_every == 0:\n",
    "            torch.save(encoder, os.path.join(models_dir, f\"encoder-{saves:04d}-{loss:.5f}.pkl\"))\n",
    "            torch.save(decoder, os.path.join(models_dir, f\"decoder-{saves:04d}-{loss:.5f}.pkl\"))\n",
    "            saves += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_style, output_style, max_length=max_sentence_length):\n",
    "    with torch.no_grad():\n",
    "        x = get_sentence_tensor(input_style, sentence)\n",
    "        x_length = x.size(0)\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(\n",
    "            max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(x_length):\n",
    "            encoder_output, encoder_hidden = encoder(x[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[start_symbol_index]], device=device)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input,\n",
    "                decoder_hidden,\n",
    "                encoder_outputs\n",
    "            )\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == end_symbol_index:\n",
    "                decoded_words.append(end_symbol)\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_style.words[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: DELETES ALL CHECKPOINTS\n",
    "if not os.path.isdir(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "for f in os.listdir(models_dir):\n",
    "    os.unlink(os.path.join(models_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new models with hidden size: 6625...\n"
     ]
    }
   ],
   "source": [
    "hidden_size = max(256, max(len(formal.words), len(casual.words)) + 1)\n",
    "print(f\"Creating new models with hidden size: {hidden_size}...\")\n",
    "pair_tensors = [get_pair_tensors(formal, casual, pair) for pair in pairs]\n",
    "\n",
    "encoder = EncoderRNN(len(formal.words), hidden_size).to(device)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size, len(casual.words)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26129/2321133138.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_iters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_26129/3840670419.py\u001b[0m in \u001b[0;36mtrain_iters\u001b[0;34m(pair_tensors, encoder, decoder, n_iters, print_every, plot_every, save_every, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         loss = train(x, desired_y, encoder, decoder,\n\u001b[0m\u001b[1;32m     18\u001b[0m                      encoder_optimizer, decoder_optimizer, criterion)\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26129/3498625035.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, desired_y, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;32m/usr/lib/python3.9/warnings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, message, category, filename, lineno, file, line, source)\u001b[0m\n\u001b[1;32m    401\u001b[0m                         \"line\", \"source\")\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     def __init__(self, message, category, filename, lineno, file=None,\n\u001b[0m\u001b[1;32m    404\u001b[0m                  line=None, source=None):\n\u001b[1;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display.display(fig, display_id=\"loss\")\n",
    "\n",
    "train_iters(pair_tensors, encoder, attn_decoder, 1000, print_every=10, plot_every=10, save_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = sorted(os.listdir(models_dir))\n",
    "encoder_models = [m for m in all_models if m.startswith(\"encoder\")]\n",
    "decoder_models = [m for m in all_models if m.startswith(\"decoder\")]\n",
    "\n",
    "encoder_model = os.path.join(models_dir, encoder_models[-1])\n",
    "decoder_model = os.path.join(models_dir, decoder_models[-1])\n",
    "\n",
    "# Ensure that the models are from the same checkpoint\n",
    "assert encoder_model.replace(\"encoder\", \"\") == decoder_model.replace(\"decoder\", \"\")\n",
    "\n",
    "print(f\"Loading encoder from '{encoder_model}'...\")\n",
    "print(f\"Loading decoder from '{decoder_model}'...\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "encoder = torch.load(encoder_model)\n",
    "decoder = torch.load(decoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"You should seek medical attention.\"\n",
    "\n",
    "print(f\"\\\"{sentence}\\\"\")\n",
    "print(\"->\")\n",
    "start = time.time()\n",
    "decoded_words, _ = evaluate(encoder, decoder, sentence, formal, casual)\n",
    "elapsed = time.time()-start\n",
    "print(decoded_words)\n",
    "print(f\"\\nTook {format_time(elapsed)}.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
